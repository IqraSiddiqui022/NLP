{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWb7W/SNiGFIXjchrXnjGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IqraSiddiqui022/NLP/blob/main/nlp_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTCWxPIBw-3H",
        "outputId": "53ac43da-344d-43bb-88e2-14163bf123d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.7.5                         \n",
            "Location         /usr/local/lib/python3.11/dist-packages/spacy\n",
            "Platform         Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Python version   3.11.11                       \n",
            "Pipelines        en_core_web_sm (3.7.1)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import spacy"
      ],
      "metadata": {
        "id": "SGyk0ZNHxCtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTg4PtxBxF0Q",
        "outputId": "6757b51c-a55b-4a06-bd16-625c1da02b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization**\n",
        "\n",
        "- **Tokenization** is the cognitive process of **breaking text into little parts** (tokens).\n",
        "1. Tokens can be **words, prison term, or even paragraphs**.\n",
        "- Example:\n",
        "\n",
        "\"I love NLP! \"\n",
        "\n",
        "Tokens: [\"I\", \"love\", \"NLP\", \"! \"]\n",
        "\n",
        "1.\n",
        "    1. **Why Tokenization?**  It facilitate in processing school text efficiently for NLP tasks.\n",
        "\n",
        "## **Iterable Objects in Python**\n",
        "\n",
        "- Lists are iterable objects, stand for they allow iteration (looping)."
      ],
      "metadata": {
        "id": "hP06QBh006oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Token Properties:**\n",
        "\n",
        "- \"didn't\" → Rive into **\"did\"** and **\"n't\"**\n",
        "1. \"₹500\" → \"₹\" and \"500\"\n",
        "2. \"Hello. \" → \"Hello\" and \". \"\n",
        "- The Doc object supports indexing and slicing like a Python list:\n",
        "\n",
        "print(doc[0]) # First Good Book\n",
        "\n",
        "print(doc[:3]) # First three Scripture"
      ],
      "metadata": {
        "id": "UvxhHIug1Eip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "NJ0UDVOTxLGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNGYCvwTvXay"
      },
      "outputs": [],
      "source": [
        "\"I love NLP!\"\n",
        "Tokens: [\"I\", \"love\", \"NLP\", \"!\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [1, 2, 3, 4]:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXRk_b1mw60K",
        "outputId": "e9418db8-6ba3-4744-9a58-edbe48144ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I love NLP!\")"
      ],
      "metadata": {
        "id": "iH8ypMOBw-Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([t.text for t in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk23aA99x7iZ",
        "outputId": "d5ce6c1e-7dbc-4d82-e441-c58fc9e0605a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'NLP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMZg6ZL9yCaI",
        "outputId": "462a27ca-d47b-4e18-d92c-7f0524beb858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(doc[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6eNJaV_yF2j",
        "outputId": "4baa9c50-3142-4f6a-c94d-ce11251b3df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.token.Token'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc[0:3])\n",
        "print(type(doc[0:3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD4ejxEIyJyX",
        "outputId": "4f80e003-2726-4c85-a7c5-4a2a6c751534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love NLP\n",
            "<class 'spacy.tokens.span.Span'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([(t.text, t.i) for t in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkIXIydUyNFh",
        "outputId": "0c4a6436-fa9f-44b3-c3e0-ae7aebf04c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 0), ('love', 1), ('NLP', 2), ('!', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7D8UFzOyTe0",
        "outputId": "6accf2d8-c42d-4294-c1df-b4bbacee88ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love NLP!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc[0])  # First word\n",
        "print(doc[:3]) # First three words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZXiZ0VXxV-_",
        "outputId": "6ed047a8-e5e1-4189-fd2c-10a5dcd928f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "I love NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"He didn't want to pay $20 for this book.\"\n",
        "doc = nlp(s)"
      ],
      "metadata": {
        "id": "NNfMGprjxxWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([t.text for t in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFrciZ2Yx3Ne",
        "outputId": "0710ee79-3bb4-4896-c88c-47e593b164e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'did', \"n't\", 'want', 'to', 'pay', '$', '20', 'for', 'this', 'book', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "s = \"\"\"Either the well was very deep, or she fell very slowly, for she\n",
        "had plenty of time as she went down to look about her and to wonder what\n",
        "was going to happen next. First, she tried to look down and make out what\n",
        "she was coming to, but it was too dark to see anything; then she looked at\n",
        "the sides of the well, and noticed that they were filled with cupboards and\n",
        "book-shelves; here and there she saw maps and pictures hung upon pegs.\"\"\"\n",
        "\n",
        "s = \"\"\"Either the well was very deep, or she fell very slowly, for she\n",
        "had plenty of time as she went down to look about her and to wonder what\n",
        "was going to happen next. First, she tried to look down and make out what\n",
        "she was coming to, but it was too dark to see anything; then she looked at\n",
        "the sides of the well, and noticed that they were filled with cupboards and\n",
        "book-shelves; here and there she saw maps and pictures hung upon pegs.\"\"\"\n",
        "\n",
        "doc = nlp(s)\n",
        "\n",
        "# Look at individual sentences (there should be two 'Span' objects).\n",
        "print([sent for sent in doc.sents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS1STv3syeky",
        "outputId": "b5786bcc-ca05-47f9-bc34-2f7f5f762356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Either the well was very deep, or she fell very slowly, for she\n",
            "had plenty of time as she went down to look about her and to wonder what\n",
            "was going to happen next., First, she tried to look down and make out what\n",
            "she was coming to, but it was too dark to see anything; then she looked at\n",
            "the sides of the well, and noticed that they were filled with cupboards and\n",
            "book-shelves; here and there she saw maps and pictures hung upon pegs.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# EXERCISE:\n",
        "# 1) Tokenize the following text\n",
        "# 2) Iterate through the tokens to check whether there's a currency symbol.\n",
        "# 3) If there is, and the currency label is followed by a number, print\n",
        "#    both the symbol and the number.\n",
        "#\n",
        "# Look through https://spacy.io/api/token#attributes on how to check whether\n",
        "# a token is a currency symbol or a number.\n",
        "#\n",
        "# Expected output: \"$20\".\n",
        "import spacy\n",
        "\n",
        "# Load the language model\n",
        "nlp2 = spacy.load('en_core_web_sm')\n",
        "\n",
        "s = \"He didn't want to pay $20 for this book.\"\n",
        "doc = nlp2(s)\n",
        "\n",
        "# Iterate through tokens and check for currency symbols and numbers\n",
        "for token in doc:\n",
        "    # Check if the token is a currency symbol using token.is_currency\n",
        "    if token.is_currency:\n",
        "        # Check if the next token is a number using token.like_num\n",
        "        if doc[token.i + 1].like_num:\n",
        "            print(token.text, doc[token.i + 1].text) # Print the symbol and number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyTcQ_BNx5MN",
        "outputId": "34858b00-a7c3-48ba-f269-69936a4a9234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = \"Let's go to N.Y.C. for the weekend.\""
      ],
      "metadata": {
        "id": "LvqR3Qahyv5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = \"He told Dr. Lovato that he was done with the tests and would post the results shortly.\"\n",
        "doc = nlp(k)"
      ],
      "metadata": {
        "id": "zFKKT5gJyztt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Case-Folding (Lowercasing)\n",
        "Convert text to lowercase for uniformity.\n",
        "Model: \"NLP is Fun\" → \"nlp is fun\"\n",
        "spaCy uses token. lower_ attribute to access lowercase text.**"
      ],
      "metadata": {
        "id": "MKfshofE0Co6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([t.lower_ for t in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US9HXgLczryP",
        "outputId": "d64d7c21-83d2-47e3-d4db-95783c025029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([t.lower_ if not t.is_sent_start else t for t in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nXXBohTz9aY",
        "outputId": "5ace2eaf-5223-480e-cecd-beb6f36d056e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[He, 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. End Word Removal\n",
        "Stop words are common word (for example, \"the\", \"is\", \"and\") that don’t carry much meaning.\n",
        "spaCy has a make-in blockage word list.\n",
        "Example:\n",
        "token. is_stop # Returns True if a souvenir is a stop give-and-take < /div>"
      ],
      "metadata": {
        "id": "oVkmnCoo0QCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# spaCy's default stop word list.\n",
        "print(nlp.Defaults.stop_words)\n",
        "print(len(nlp.Defaults.stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51OEzOUz0IKe",
        "outputId": "b09e26ec-c76c-4242-f039-57be5dfddfb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'however', 'ours', 'nowhere', 'twelve', 'for', 'well', 'himself', 'must', 'about', 'who', 'yourself', 'at', 'unless', 'across', 'whereafter', 'does', 'just', 'are', 'more', 'whereas', 'did', 'itself', 'until', 'none', 'say', 'from', 'part', 'go', 'with', 'by', 'formerly', 'these', 'than', 'be', 'whole', 'under', 'around', 'after', 'here', 'fifty', 'whereby', '‘ve', 'perhaps', 'many', 'might', 'various', 'regarding', 'toward', 'latter', 'hereupon', 'move', 'herein', 'he', 'whom', 'therein', 'two', 'get', 'into', 'six', 'because', 'hers', 'and', 'sometime', 'thereby', 'been', 'can', 'this', 'therefore', \"'re\", 'thru', 'they', 'nobody', 'below', 'first', 'there', 'we', 'please', 'name', 'becoming', 'nine', 'will', \"n't\", '‘ll', 'using', 'became', 'whose', 'via', 'was', 'once', 'during', 'next', 'as', 'due', '‘d', 'less', 'whatever', 'hundred', 'my', 'n’t', 'mine', 'whither', 'top', '’d', 'your', 'its', 'other', 'alone', 'if', 'take', 'put', 'should', 'somehow', 'then', 'doing', 'nor', 'anything', 'very', 'call', 'our', 'neither', 'over', 'his', 'throughout', 'made', 'further', 'show', 'become', 'done', 'is', 'wherein', 're', 'thus', 'another', 'has', 'somewhere', 'seems', 'though', 'towards', 'anyhow', 'own', 'within', 'four', 'the', 'third', 'i', 'whereupon', 'seeming', 'up', 'being', 'rather', 'even', '’ll', 'herself', 'thereafter', 'make', 'every', 'out', '‘s', 'seem', 'ca', 'or', 'something', 'each', 'am', 'a', 'empty', '‘re', 'had', 'beyond', \"'ve\", 'do', 'above', 'yourselves', 'ever', 'besides', 'five', 'otherwise', 'where', 'through', 'front', 'she', 'would', 'whence', 'almost', 'their', 'thereupon', 'ten', 'themselves', 'except', 'several', 'off', 'give', 'whether', 'quite', 'least', 'keep', 'eleven', 'so', 'onto', 'why', 'same', 'not', 'beforehand', 'much', 'one', 'although', 'amongst', 'full', 'without', 'never', 'on', 'elsewhere', 'that', 'nothing', 'along', 'again', 'of', 'me', 'afterwards', '‘m', 'how', 'too', 'them', 'seemed', 'side', 'anyone', 'nevertheless', 'still', 'also', 'everywhere', 'only', 'sixty', 'enough', 'bottom', '’s', 'forty', 'upon', 'noone', 'everyone', 'were', 'her', 'already', 'else', 'ourselves', 'latterly', 'may', 'but', 'three', 'any', 'no', 'often', 'back', 'now', 'everything', 'us', 'when', 'an', 'twenty', 'hence', 'between', 'most', 'to', \"'s\", 'whenever', 'which', 'few', 'down', 'others', 'someone', 'meanwhile', 'fifteen', 'cannot', 'last', 'what', 'have', 'amount', 'those', 'wherever', 'hereafter', 'serious', 'former', 'in', \"'m\", '’ve', 'behind', 'could', 'yours', 'see', '’m', 'mostly', 'anyway', 'per', 'either', 'him', 'such', 'both', 'it', 'sometimes', 'before', 'always', 'you', 'since', 'thence', 'n‘t', '’re', 'indeed', 'all', 'anywhere', \"'d\", 'whoever', \"'ll\", 'beside', 'while', 'among', 'becomes', 'eight', 'myself', 'really', 'against', 'hereby', 'used', 'together', 'yet', 'moreover', 'namely', 'some'}\n",
            "326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "print([t for t in doc if not t.is_stop])"
      ],
      "metadata": {
        "id": "aDMCW5LZ0eKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Lemmatization vs. Stemming**\n",
        "\n",
        "Feature Stemming Lemmatization\n",
        "\n",
        "Definition\n",
        "\n",
        "Removes prefix & suffixes\n",
        "\n",
        "Happen dictionary-based root word\n",
        "\n",
        "Example\n",
        "\n",
        "\"changing\" → \"chang\"\n",
        "\n",
        "\"changing\" → \"change\"\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Less precise\n",
        "\n",
        "More accurate\n",
        "\n",
        "Method\n",
        "\n",
        "Rule-based\n",
        "\n",
        "Dictionary-based\n",
        "\n",
        "- **spaCy supports lemmatization using token. lemma_**\n",
        "- **NLTK** render stem with the  **Snowball Stemmer**."
      ],
      "metadata": {
        "id": "O4ttHev40mgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(t.text, t.lemma_) for t in doc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz-uESuo0eo6",
        "outputId": "87248a2d-9b84-426a-ba78-2f0dbf94f5f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('He', 'he'),\n",
              " ('told', 'tell'),\n",
              " ('Dr.', 'Dr.'),\n",
              " ('Lovato', 'Lovato'),\n",
              " ('that', 'that'),\n",
              " ('he', 'he'),\n",
              " ('was', 'be'),\n",
              " ('done', 'do'),\n",
              " ('with', 'with'),\n",
              " ('the', 'the'),\n",
              " ('tests', 'test'),\n",
              " ('and', 'and'),\n",
              " ('would', 'would'),\n",
              " ('post', 'post'),\n",
              " ('the', 'the'),\n",
              " ('results', 'result'),\n",
              " ('shortly', 'shortly'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**basic processing**"
      ],
      "metadata": {
        "id": "BwCnOr2-1gKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "s = 'He told Dr. Lovato that he was done with the tests and would post the results shortly.'"
      ],
      "metadata": {
        "id": "dn1WC5YU0pow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advanced Text Processing Techniques**\n",
        "\n",
        "### **1. Part-of-Speech (POS) Tagging**\n",
        "\n",
        "- Identifies words as **nouns, verb, adjectives, etc.**\n",
        "    - Instance: \"The cat posture on the mat. \"\"cat\" → Noun\n",
        "    - \"sat\" → Verb"
      ],
      "metadata": {
        "id": "Ih1wXgyMuAaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "s = \"John watched an old movie at the cinema.\"\n",
        "doc = nlp(s)"
      ],
      "metadata": {
        "id": "vYdJ89yE1rQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "511d421d-d3bd-4004-fb26-03275a712a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[(t.text, t.pos_) for t in doc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMC60sr8tzWQ",
        "outputId": "3dc15954-bdd2-400b-94ab-64aa0b3f04c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'PROPN'),\n",
              " ('watched', 'VERB'),\n",
              " ('an', 'DET'),\n",
              " ('old', 'ADJ'),\n",
              " ('movie', 'NOUN'),\n",
              " ('at', 'ADP'),\n",
              " ('the', 'DET'),\n",
              " ('cinema', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('PROPN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tm6dTuJMuKJ8",
        "outputId": "1e81ec8f-9dce-4609-e4cf-ae052215ddff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'proper noun'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[(t.text, t.tag_) for t in doc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwF_fMHVuLQe",
        "outputId": "f87afd64-2d71-4f0e-b985-f088d43f4700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'NNP'),\n",
              " ('watched', 'VBD'),\n",
              " ('an', 'DT'),\n",
              " ('old', 'JJ'),\n",
              " ('movie', 'NN'),\n",
              " ('at', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('cinema', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spacy.explain('NNP'))\n",
        "print(spacy.explain('VBD'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZrLZGGguPGz",
        "outputId": "29ec19eb-3187-44df-a0da-6ff0ee892f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noun, proper singular\n",
            "verb, past tense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Distinguish Entity Recognition (NER)**\n",
        "\n",
        "- **Tell Apart entities** like names, appointment, and places.\n",
        "    - Example: \"Elon Musk notice Tesla in 2003. \"\"Elon Musk\" → **Person**\n",
        "    - \"Tesla\" → **Organization**\n",
        "    - \"2003\" → Date\n",
        "    "
      ],
      "metadata": {
        "id": "aogzoqCOuwQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Volkswagen is developing an electric sedan which could potentially come to America next fall.\"\n",
        "doc = nlp(s)\n",
        "\n",
        "[(t.text, t.ent_type_) for t in doc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92tN2yh6uZNn",
        "outputId": "d64ea683-3314-42f9-fa05-5b2d6f1ca7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Volkswagen', 'ORG'),\n",
              " ('is', ''),\n",
              " ('developing', ''),\n",
              " ('an', ''),\n",
              " ('electric', ''),\n",
              " ('sedan', ''),\n",
              " ('which', ''),\n",
              " ('could', ''),\n",
              " ('potentially', ''),\n",
              " ('come', ''),\n",
              " ('to', ''),\n",
              " ('America', 'GPE'),\n",
              " ('next', 'DATE'),\n",
              " ('fall', 'DATE'),\n",
              " ('.', '')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# We need to set the 'jupyter' variable to True in order to output\n",
        "# the visualization directly. Otherwise, you'll get raw HTML.\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "4iotTd6CuaOg",
        "outputId": "0daabf2a-682f-4774-975b-441269bfb04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Volkswagen\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is developing an electric sedan which could potentially come to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    America\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    next fall\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Ridley Scott directed The Martian.\"\n",
        "doc = nlp(s)\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "WJwmR9htuadf",
        "outputId": "918b77c6-2212-4236-d5be-e168394d9460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ridley Scott\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " directed The \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Martian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"She enrolled in the course at the university.\"\n",
        "doc = nlp(s)\n",
        "\n",
        "# Note the 'style' argument is assigned a 'dep' flag this time around.\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "adF7_-Bduppy",
        "outputId": "d955a68d-39ff-4628-a4d5-1fd638dce6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"094b1cf020c5459c92097d90e0af6145-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">She</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">enrolled</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">course</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">at</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">university.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-094b1cf020c5459c92097d90e0af6145-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-094b1cf020c5459c92097d90e0af6145-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-094b1cf020c5459c92097d90e0af6145-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-094b1cf020c5459c92097d90e0af6145-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-094b1cf020c5459c92097d90e0af6145-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-094b1cf020c5459c92097d90e0af6145-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-094b1cf020c5459c92097d90e0af6145-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-094b1cf020c5459c92097d90e0af6145-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-094b1cf020c5459c92097d90e0af6145-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-094b1cf020c5459c92097d90e0af6145-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-094b1cf020c5459c92097d90e0af6145-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-094b1cf020c5459c92097d90e0af6145-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-094b1cf020c5459c92097d90e0af6145-0-6\" stroke-width=\"2px\" d=\"M945,177.0 C945,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-094b1cf020c5459c92097d90e0af6145-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('nsubj')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zk8Bzv7tvD5R",
        "outputId": "014cca9f-c182-4ce6-f369-d2755d523e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nominal subject'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[(t.text, t.dep_) for t in doc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMnp-EozvL4U",
        "outputId": "4530b999-6191-4cda-ceb0-9bcced046b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('She', 'nsubj'),\n",
              " ('enrolled', 'ROOT'),\n",
              " ('in', 'prep'),\n",
              " ('the', 'det'),\n",
              " ('course', 'pobj'),\n",
              " ('at', 'prep'),\n",
              " ('the', 'det'),\n",
              " ('university', 'pobj'),\n",
              " ('.', 'punct')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The general Matcher is one of multiple matcher objects\n",
        "# included with spaCy.\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# We initialize the Matcher with the spaCy vocab object, which contains\n",
        "# words along with their labels and entities.\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "s = \"I want to book a hotel room.\"\n",
        "doc = nlp(s)\n",
        "\n",
        "# Patterns are expressed as an ordered sequence. Here, we're looking\n",
        "# to match occurrences starting with a 'book' string followed by\n",
        "# a determiner (DET) POS tag, then a noun POS tag.\n",
        "# The OP key marks the match as optional in some way.\n",
        "\n",
        "# Here, the DET POS (marked with '?') will match 0 or 1 times, and\n",
        "# the NOUN POS (marked with '+') will match 1 or more times.\n",
        "# See this link for more information:\n",
        "# https://spacy.io/usage/rule-based-matching#quantifiers\n",
        "pattern = [\n",
        "  {'TEXT': 'book'},\n",
        "  {'POS': 'DET', 'OP': '?'},\n",
        "  {'POS': 'NOUN', 'OP': '+'},\n",
        "]\n",
        "\n",
        "# We give our pattern a label and pass it to the matcher.\n",
        "matcher.add('USER_INTENT', [pattern])\n",
        "\n",
        "# Run the matcher over the doc.\n",
        "matches = matcher(doc)\n",
        "\n",
        "# For each match, the matcher returns a tuple specifying a match id, start,\n",
        "# and end of the match.\n",
        "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA9lODFavNwK",
        "outputId": "53c17ac3-791e-4176-9598-b39bbbf8d215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches: ['book a hotel', 'book a hotel room']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yoda's Form (Object-Subject-verb)\n",
        "\"You will learn NLP.\"\n",
        "\n",
        "How does this work\n",
        "In the interest of NLP, dependency parsing is an NLP technique for determining the structure of a sentence.  // Reorder the words based on their dependency roles (word order of an English sentence can be re-arranged if subject,verb and object plays same role\n",
        "\n",
        "As Yoda,** tends to put objects first, so we order the sentence like this."
      ],
      "metadata": {
        "id": "u60mHMyXvyPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yodize(s: str):\n",
        "  doc = nlp(s)\n",
        "  for t in doc:\n",
        "    if t.dep_ == \"ROOT\":\n",
        "\n",
        "      # Assuming our sentence is of the form subject-verb-object, we take\n",
        "      # everything after the root (likely verb) and put it in front, and\n",
        "      # likewise take everything before the root, and put it after.\n",
        "      seq = [doc[t.i + 1: -1].text, doc[0: t.i].text, t.text + '.']\n",
        "      seq[0] = seq[0].capitalize()\n",
        "      print(' '.join(seq))"
      ],
      "metadata": {
        "id": "g_CQtb7evXXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"We'll be in Osaka on Feb 13th and leave on Feb 24th.\"\n",
        "doc = nlp(s)"
      ],
      "metadata": {
        "id": "NLqPcTO9vguR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Using the PhraseMatcher, find the start and end index of all occurrences\n",
        "# of 'Caesar Augustus' and 'Roman Empire' (case-insensitive).\n",
        "#\n",
        "# Expected output: [(0, 2), (15, 17)]\n",
        "#\n",
        "from spacy.matcher import PhraseMatcher\n",
        "s = \"Caesar Augustus was the founder of the Roman Principate (the first phase of the Roman Empire).\"\n",
        "doc = nlp(s)"
      ],
      "metadata": {
        "id": "VqdlMGM9v3Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Improving bowing: N-Grams**\n",
        "\n",
        "- **N-grams** gaining control news sequences.\n",
        "    - **Deterrent Example of bigrams (n=2):**\"Natural Language\"\n",
        "    - \"Language Processing\"\n",
        "- In **CountVectorizer**, dress ngram_range=(1, 2) to get unigrams & bigrams.\n",
        "\n",
        "## **Custom Tokenization in BoW**\n",
        "\n",
        "- We can use a custom tokenizer in CountVectorizer.\n",
        "- Example utilize spaCy:\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, binary=True)\n",
        "\n",
        "bow = vectorizer. fit_transform(corpus)\n",
        "\n",
        "- **Binary BOW** → Only computer memory **presence (1) or absence (0) of words**, not frequency."
      ],
      "metadata": {
        "id": "Nhzs2PLRxjbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "from scipy import spatial\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "mlyaRhqWv7lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# A corpus of sentences.\n",
        "corpus = [\n",
        "  \"Red Bull drops hint on F1 engine.\",\n",
        "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
        "  \"Hamilton eyes record eighth F1 title.\",\n",
        "  \"Aston Martin announces sponsor.\"\n",
        "]"
      ],
      "metadata": {
        "id": "Z6R_QIHrwIlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n"
      ],
      "metadata": {
        "id": "OzxV4ikdwcYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow = vectorizer.fit_transform(corpus)\n"
      ],
      "metadata": {
        "id": "6-0MUT4gwiSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View features (tokens).\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# View vocabulary dictionary.\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBGALG7Swk3E",
        "outputId": "d9009fea-d1b7-4292-ea3b-85ffeef24d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['announces' 'aston' 'bull' 'drops' 'eighth' 'engine' 'exits' 'eyes' 'f1'\n",
            " 'hamilton' 'hint' 'honda' 'leaving' 'martin' 'on' 'partner' 'record'\n",
            " 'red' 'sponsor' 'title']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'red': 17,\n",
              " 'bull': 2,\n",
              " 'drops': 3,\n",
              " 'hint': 10,\n",
              " 'on': 14,\n",
              " 'f1': 8,\n",
              " 'engine': 5,\n",
              " 'honda': 11,\n",
              " 'exits': 6,\n",
              " 'leaving': 12,\n",
              " 'partner': 15,\n",
              " 'hamilton': 9,\n",
              " 'eyes': 7,\n",
              " 'record': 16,\n",
              " 'eighth': 4,\n",
              " 'title': 19,\n",
              " 'aston': 1,\n",
              " 'martin': 13,\n",
              " 'announces': 0,\n",
              " 'sponsor': 18}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(bow))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6JnJAHlwoSN",
        "outputId": "584d2494-d860-425f-9b2a-d4adc0a43e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC97oF3Kxmkj",
        "outputId": "d9ba81ba-24af-4e22-c228-76febfa6bcb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 17)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 14)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 5)\t1\n",
            "  (1, 17)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 8)\t2\n",
            "  (1, 11)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 12)\t1\n",
            "  (1, 15)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 16)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 19)\t1\n",
            "  (3, 1)\t1\n",
            "  (3, 13)\t1\n",
            "  (3, 0)\t1\n",
            "  (3, 18)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As usual, we start by importing spaCy and loading a statistical model.\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a tokenizer callback using spaCy under the hood. Here, we tokenize\n",
        "# the passed-in text and return the tokens, filtering out punctuation.\n",
        "def spacy_tokenizer(doc):\n",
        "  return [t.text for t in nlp(doc) if not t.is_punct]"
      ],
      "metadata": {
        "id": "wxnsq_-6xpJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True)\n",
        "bow = vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7ukQh83xr9p",
        "outputId": "32aae5e7-e6e2-40f4-e713-c138b0de4ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHOmqI3RxvFY",
        "outputId": "4cce82b8-d082-457c-8b90-eb222ded98e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Aston' 'Bull' 'F1' 'Hamilton' 'Honda' 'Martin' 'Red' 'announces' 'drops'\n",
            " 'eighth' 'engine' 'exits' 'eyes' 'hint' 'leaving' 'on' 'partner' 'record'\n",
            " 'sponsor' 'title']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Red': 6,\n",
              " 'Bull': 1,\n",
              " 'drops': 8,\n",
              " 'hint': 13,\n",
              " 'on': 15,\n",
              " 'F1': 2,\n",
              " 'engine': 10,\n",
              " 'Honda': 4,\n",
              " 'exits': 11,\n",
              " 'leaving': 14,\n",
              " 'partner': 16,\n",
              " 'Hamilton': 3,\n",
              " 'eyes': 12,\n",
              " 'record': 17,\n",
              " 'eighth': 9,\n",
              " 'title': 19,\n",
              " 'Aston': 0,\n",
              " 'Martin': 5,\n",
              " 'announces': 7,\n",
              " 'sponsor': 18}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('A dense representation like we saw in the slides.')\n",
        "print(bow.toarray())\n",
        "print()\n",
        "print('Indexing and slicing.')\n",
        "print(bow[0])\n",
        "print()\n",
        "print(bow[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocM3z5TgxwqT",
        "outputId": "d042b0d1-f4b8-406a-a625-9ef4aa60efd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A dense representation like we saw in the slides.\n",
            "[[0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            " [0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0]\n",
            " [0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1]\n",
            " [1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
            "\n",
            "Indexing and slicing.\n",
            "  (0, 6)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 10)\t1\n",
            "\n",
            "  (0, 6)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 10)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 11)\t1\n",
            "  (1, 14)\t1\n",
            "  (1, 16)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cosine Similarity**\n",
        "\n",
        "- **Measuring Rod text similarity** between documents.\n",
        "- **Formula:** cos⁡(θ)=A⋅B∣∣A∣∣∣∣B∣∣\\cos(\\theta) = \\frac{A \\cdot B}{||A|| ||B||}\n",
        "- **Implementation habituate scipy. spatial. distance**:\n",
        "from scipy. spatial. distance importee cos\n",
        "similarity = 1 - cosine(vector1, vector2)\n",
        "\n",
        "-\n",
        "    - **Applications:**Search engines\n",
        "    - Recommendation systems\n",
        "    - Plagiarism detection"
      ],
      "metadata": {
        "id": "oXgqYr6Ix9ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine"
      ],
      "metadata": {
        "id": "yGXi8wNfyJQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The cosine method expects array_like inputs, so we need to generate\n",
        "# arrays from our sparse matrix.\n",
        "doc1_vs_doc2 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[1].toarray()[0])\n",
        "doc1_vs_doc3 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[2].toarray()[0])\n",
        "doc1_vs_doc4 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[3].toarray()[0])\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "print(f\"Doc 1 vs Doc 2: {doc1_vs_doc2}\")\n",
        "print(f\"Doc 1 vs Doc 3: {doc1_vs_doc3}\")\n",
        "print(f\"Doc 1 vs Doc 4: {doc1_vs_doc4}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ajEqv-lxzjy",
        "outputId": "33dec94d-b40c-494d-d7fd-dcf804903d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Red Bull drops hint on F1 engine.', 'Honda exits F1, leaving F1 partner Red Bull.', 'Hamilton eyes record eighth F1 title.', 'Aston Martin announces sponsor.']\n",
            "Doc 1 vs Doc 2: 0.4285714285714286\n",
            "Doc 1 vs Doc 3: 0.15430334996209194\n",
            "Doc 1 vs Doc 4: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(1,2))\n",
        "bigrams = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print('Number of features: {}'.format(len(vectorizer.get_feature_names_out())))\n",
        "print(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2HGp20yyC29",
        "outputId": "3e2d850e-7cf8-4ab9-e8d5-052291bb8ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Aston' 'Aston Martin' 'Bull' 'Bull drops' 'F1' 'F1 engine' 'F1 leaving'\n",
            " 'F1 partner' 'F1 title' 'Hamilton' 'Hamilton eyes' 'Honda' 'Honda exits'\n",
            " 'Martin' 'Martin announces' 'Red' 'Red Bull' 'announces'\n",
            " 'announces sponsor' 'drops' 'drops hint' 'eighth' 'eighth F1' 'engine'\n",
            " 'exits' 'exits F1' 'eyes' 'eyes record' 'hint' 'hint on' 'leaving'\n",
            " 'leaving F1' 'on' 'on F1' 'partner' 'partner Red' 'record'\n",
            " 'record eighth' 'sponsor' 'title']\n",
            "Number of features: 40\n",
            "{'Red': 15, 'Bull': 2, 'drops': 19, 'hint': 28, 'on': 32, 'F1': 4, 'engine': 23, 'Red Bull': 16, 'Bull drops': 3, 'drops hint': 20, 'hint on': 29, 'on F1': 33, 'F1 engine': 5, 'Honda': 11, 'exits': 24, 'leaving': 30, 'partner': 34, 'Honda exits': 12, 'exits F1': 25, 'F1 leaving': 6, 'leaving F1': 31, 'F1 partner': 7, 'partner Red': 35, 'Hamilton': 9, 'eyes': 26, 'record': 36, 'eighth': 21, 'title': 39, 'Hamilton eyes': 10, 'eyes record': 27, 'record eighth': 37, 'eighth F1': 22, 'F1 title': 8, 'Aston': 0, 'Martin': 13, 'announces': 17, 'sponsor': 38, 'Aston Martin': 1, 'Martin announces': 14, 'announces sponsor': 18}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting n_gram range to (2, 2) generates only bigrams.\n",
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(2,2))\n",
        "bigrams = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf1H-_o8ydlE",
        "outputId": "af858865-7842-4823-ed5f-90a312912a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Aston Martin' 'Bull drops' 'F1 engine' 'F1 leaving' 'F1 partner'\n",
            " 'F1 title' 'Hamilton eyes' 'Honda exits' 'Martin announces' 'Red Bull'\n",
            " 'announces sponsor' 'drops hint' 'eighth F1' 'exits F1' 'eyes record'\n",
            " 'hint on' 'leaving F1' 'on F1' 'partner Red' 'record eighth']\n",
            "{'Red Bull': 9, 'Bull drops': 1, 'drops hint': 11, 'hint on': 15, 'on F1': 17, 'F1 engine': 2, 'Honda exits': 7, 'exits F1': 13, 'F1 leaving': 3, 'leaving F1': 16, 'F1 partner': 4, 'partner Red': 18, 'Hamilton eyes': 6, 'eyes record': 14, 'record eighth': 19, 'eighth F1': 12, 'F1 title': 5, 'Aston Martin': 0, 'Martin announces': 8, 'announces sponsor': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bag of Words (arc) Model**\n",
        "\n",
        "- Represents **documents as numerical vectors**.\n",
        "- Each row = a document, each chromatography column = a tidings count."
      ],
      "metadata": {
        "id": "WrMufUPLymf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE: Create a spacy_tokenizer callback which takes a string and returns\n",
        "# a list of tokens (each token's text) with punctuation filtered out.\n",
        "#\n",
        "corpus = [\n",
        "  \"Students use their GPS-enabled cellphones to take birdview photographs of a land in order to find specific danger points such as rubbish heaps.\",\n",
        "  \"Teenagers are enthusiastic about taking aerial photograph in order to study their neighbourhood.\",\n",
        "  \"Aerial photography is a great way to identify terrestrial features that aren’t visible from the ground level, such as lake contours or river paths.\",\n",
        "  \"During the early days of digital SLRs, Canon was pretty much the undisputed leader in CMOS image sensor technology.\",\n",
        "  \"Syrian President Bashar al-Assad tells the US it will 'pay the price' if it strikes against Syria.\"\n",
        "]\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def spacy_tokenizer(doc):\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "luHl8zykygBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = [\"Teenagers take aerial shots of their neighbourhood using digital cameras sitting in old bottles which are launched via kites - a common toy for children living in the favelas. They then use GPS-enabled smartphones to take pictures of specific danger points - such as rubbish heaps, which can become a breeding ground for mosquitoes carrying dengue fever.\"]\n",
        "new_bow = vectorizer.transform(s)\n",
        "import numpy as np\n",
        "def cos_sim(a, b):\n",
        "  pass"
      ],
      "metadata": {
        "id": "g5El2kx2yjop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "TF-IDF is a **Natural Language Processing (NLP)** term in which we use to find the importance of a word in any document over a set documents.\n",
        "\n",
        "### **Key Features:**\n",
        "\n",
        "- Allows the identification of **highly relevant terms** in a document\n",
        "- **Reduces common** (e.g., 'the', 'is') words occurences in document.\n",
        "Used for:\n",
        "- Search engines (to order results by relevance)\n",
        "- **Text classification** (to help in better document categorization)\n",
        "- **Keyword extraction** (summarizing the important content)\n",
        "- **Information retrieval & sharing**"
      ],
      "metadata": {
        "id": "-hOr7KALy4dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy==3.*\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy info\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhYAYQrkyvly",
        "outputId": "44e7d014-5514-4c97-ba66-647087886b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy==3.* in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Collecting spacy==3.*\n",
            "  Downloading spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (3.0.9)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy==3.*)\n",
            "  Downloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.*) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.*) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.*) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.*) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.*) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (2024.12.14)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy==3.*)\n",
            "  Downloading blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy==3.*) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.*) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.*) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.*) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.*) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.*) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy==3.*) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.*) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.*) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.*) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.*) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.*) (0.1.2)\n",
            "Downloading spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: blis, thinc, spacy\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.2.0 spacy-3.8.4 thinc-8.3.4\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.7.1\n",
            "    Uninstalling en-core-web-sm-3.7.1:\n",
            "      Successfully uninstalled en-core-web-sm-3.7.1\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.8.4                         \n",
            "Location         /usr/local/lib/python3.11/dist-packages/spacy\n",
            "Platform         Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Python version   3.11.11                       \n",
            "Pipelines        en_core_web_sm (3.8.0)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "corpus = fetch_20newsgroups(categories=['sci.space'],\n",
        "                            remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "VJ0HNGiSy7A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPmClY7G6Pi9",
        "outputId": "70c3eb43-26c5-4696-bf76-df8695e42156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.utils._bunch.Bunch'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of posts in our dataset.\n",
        "len(corpus.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNGV2teb7l4Y",
        "outputId": "0ed1f1b2-90a2-4c7b-9065-46d699427c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "593"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of posts in our dataset.\n",
        "len(corpus.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0aSKKWS7rqU",
        "outputId": "4098ed39-58c6-429a-9960-0bd8de0d547c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "593"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating TF-IDF features**"
      ],
      "metadata": {
        "id": "1UaHoXx275nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "4sqB0f_v8weB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Like before, if we want to use spaCy's tokenizer, we need\n",
        "# to create a callback. Remember to upgrade spaCy if you need\n",
        "# to (refer to beginnning of file for commentary and instructions).\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# We don't need named-entity recognition nor dependency parsing for\n",
        "# this so these components are disabled. This will speed up the\n",
        "# pipeline. We do need part-of-speech tagging however.\n",
        "unwanted_pipes = [\"ner\", \"parser\"]\n",
        "\n",
        "# For this exercise, we'll remove punctuation and spaces (which\n",
        "# includes newlines), filter for tokens consisting of alphabetic\n",
        "# characters, and return the lemma (which require POS tagging).\n",
        "def spacy_tokenizer(doc):\n",
        "  with nlp.disable_pipes(*unwanted_pipes):\n",
        "    return [t.lemma_ for t in nlp(doc) if \\\n",
        "            not t.is_punct and \\\n",
        "            not t.is_space and \\\n",
        "            t.is_alpha]"
      ],
      "metadata": {
        "id": "rnZbExfD7ylf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Example function (Replace with your actual function)\n",
        "def example_function():\n",
        "    return \"Hello, World!\"\n",
        "\n",
        "result = example_function()  # Call a real function\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Execution time:\", end_time - start_time, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCz124KA727L",
        "outputId": "129ea646-b1c6-45a2-d812-819b961cdc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: 9.417533874511719e-05 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy\n",
        "\n",
        "# Like before, if we want to use spaCy's tokenizer, we need\n",
        "# to create a callback. Remember to upgrade spaCy if you need\n",
        "# to (refer to beginnning of file for commentary and instructions).\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# We don't need named-entity recognition nor dependency parsing for\n",
        "# this so these components are disabled. This will speed up the\n",
        "# pipeline. We do need part-of-speech tagging however.\n",
        "unwanted_pipes = [\"ner\", \"parser\"]\n",
        "\n",
        "# For this exercise, we'll remove punctuation and spaces (which\n",
        "# includes newlines), filter for tokens consisting of alphabetic\n",
        "# characters, and return the lemma (which require POS tagging).\n",
        "def spacy_tokenizer(doc):\n",
        "  with nlp.disable_pipes(*unwanted_pipes):\n",
        "    return [t.lemma_ for t in nlp(doc) if \\\n",
        "            not t.is_punct and \\\n",
        "            not t.is_space and \\\n",
        "            t.is_alpha]\n",
        "\n",
        "# Use the default settings of TfidfVectorizer.\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)  # Define vectorizer here\n",
        "features = vectorizer.fit_transform(corpus.data)\n",
        "\n",
        "# The number of unique tokens.\n",
        "print(len(vectorizer.get_feature_names_out())) # Now 'vectorizer' is accessible\n",
        "print(len(vectorizer.get_feature_names_out()))"
      ],
      "metadata": {
        "id": "vqDQoHZ77_Ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b77f474-b6e4-444e-99d6-942beaab97cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9463\n",
            "9463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\"This is an example sentence\", \"Another document here\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "features = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(features.shape)"
      ],
      "metadata": {
        "id": "wpKhB6fr9FFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86799d4-0262-426e-9460-7268f8e1e7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the query into a TF-IDF vector.\n",
        "query = [\"lunar orbit\"]\n",
        "query_tfidf = vectorizer.transform(query)"
      ],
      "metadata": {
        "id": "FOeq1Bs49HJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_tokenizer(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n"
      ],
      "metadata": {
        "id": "1c2TVZWp-QM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_tokenizer(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n"
      ],
      "metadata": {
        "id": "bKTDdOcM9fqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# numpy's argsort() method returns a list of *indices* that\n",
        "# would sort an array:\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
        "#\n",
        "# The sort is ascending, but we want the largest k cosine_similarites\n",
        "# at the bottom of the sort. So we negate k, and get the last k\n",
        "# entries of the indices list in reverse order. There are faster\n",
        "# ways to do this using things like argpartition but this is\n",
        "# more succinct.\n",
        "def top_k(arr, k):\n",
        "  kth_largest = (k + 1) * -1\n",
        "  return np.argsort(arr)[:kth_largest:-1]"
      ],
      "metadata": {
        "id": "-CFjCmwF9j6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Assuming 'features' is the TF-IDF matrix\n",
        "cosine_similarities = cosine_similarity(features, features)\n"
      ],
      "metadata": {
        "id": "12LQkL-_-U2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\"This is an example sentence\", \"Another document is here\", \"This example is different\"]\n",
        "\n",
        "# Vectorizing the text\n",
        "vectorizer = TfidfVectorizer()\n",
        "features = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Compute cosine similarity\n",
        "cosine_similarities = cosine_similarity(features, features)\n"
      ],
      "metadata": {
        "id": "5DaeRK8n-XEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k(similarities, k):\n",
        "    return np.argsort(similarities, axis=1)[:, -k:]  # Get indices of top k values\n"
      ],
      "metadata": {
        "id": "JaR-4vnU-wAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_related_indices = top_k(cosine_similarities, 5)\n",
        "print(top_related_indices)  # Check if this prints valid indices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ezyT5r__lYq",
        "outputId": "92d82297-ddc7-4810-f317-a595b30f6a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 0]\n",
            " [0 2 1]\n",
            " [1 0 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Cosine Similarities:\\n\", cosine_similarities)\n",
        "print(\"Top Related Indices:\\n\", top_related_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FvBKlsL_nyz",
        "outputId": "287341b8-379c-4ee1-8015-6fb6d2bad20b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarities:\n",
            " [[1.         0.10180788 0.50801465]\n",
            " [0.10180788 1.         0.12042206]\n",
            " [0.50801465 0.12042206 1.        ]]\n",
            "Top Related Indices:\n",
            " [[1 2 0]\n",
            " [0 2 1]\n",
            " [1 0 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cosine_similarities[top_related_indices])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3whnw-i5_ruN",
        "outputId": "7030a841-fd03-4673-bcef-dec3196ed759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.10180788 1.         0.12042206]\n",
            "  [0.50801465 0.12042206 1.        ]\n",
            "  [1.         0.10180788 0.50801465]]\n",
            "\n",
            " [[1.         0.10180788 0.50801465]\n",
            "  [0.50801465 0.12042206 1.        ]\n",
            "  [0.10180788 1.         0.12042206]]\n",
            "\n",
            " [[0.10180788 1.         0.12042206]\n",
            "  [1.         0.10180788 0.50801465]\n",
            "  [0.50801465 0.12042206 1.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Features shape:\", features.shape)\n",
        "print(\"Query shape:\", query_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqklw7lQ_znL",
        "outputId": "f3c9149b-12d5-421c-d61e-232e8a0ee7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (3, 9)\n",
            "Query shape: (1, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample corpus (your dataset)\n",
        "corpus = [\"This is a sample sentence\", \"Another example sentence\", \"Machine learning is powerful\"]\n",
        "\n",
        "# Initialize the vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "features = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n"
      ],
      "metadata": {
        "id": "oSZIVzPc-Q4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"sample sentence\"\n",
        "query_tfidf = tfidf_vectorizer.transform([query])\n"
      ],
      "metadata": {
        "id": "MY8TwPxD-Qtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Features shape:\", features.shape)\n",
        "print(\"Query shape:\", query_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "vVaQwq0a-pgk",
        "outputId": "0769122e-8194-48cc-a78f-5755f9ecdd77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (3, 9)\n",
            "Query shape: (1, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(features.shape)\n",
        "print(query_tfidf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw1AKHNP_7di",
        "outputId": "63af26e7-d898-499e-a3b3-fa7e876a250d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 9)\n",
            "(1, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = features.reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "74y_-IZEBA66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cosine_similarities.shape)\n",
        "print(cosine_similarities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyCU805lBENy",
        "outputId": "9dcb095c-dc32-4cc2-e169-49c88773004b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 3)\n",
            "[[1.         0.10180788 0.50801465]\n",
            " [0.10180788 1.         0.12042206]\n",
            " [0.50801465 0.12042206 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim==4.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5WPT956BItb",
        "outputId": "22f387d2-c421-4a4f-d234-7a1edbf78653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.* in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.*) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "sE2-G9fuBiFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 word2vec_model.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBynpFG2HFW3",
        "outputId": "52044dae-2abb-41ae-f942-06649697cdd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head: cannot open 'word2vec_model.bin' for reading: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://drive.google.com/uc?export=download&id=10gx1KcFxhVo0aWv-duCyyNe1wKZE3Mo\" -O word2vec_model.bin\n",
        "# Downloads the file from the given URL and saves it as 'word2vec_model.bin'.\n",
        "# You need to find the actual URL of the file.\n",
        "\n",
        "import os\n",
        "print(os.path.getsize(\"word2vec_model.bin\"))\n",
        "# Now, this should work as the file is downloaded.\n"
      ],
      "metadata": {
        "id": "4lt6_img_bmV",
        "outputId": "0ef119eb-45da-4918-90d8-711da2000134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-06 12:19:47--  https://drive.google.com/uc?export=download&id=10gx1KcFxhVo0aWv-duCyyNe1wKZE3Mo\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.179.100, 142.251.179.102, 142.251.179.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.179.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=10gx1KcFxhVo0aWv-duCyyNe1wKZE3Mo&export=download [following]\n",
            "--2025-02-06 12:19:47--  https://drive.usercontent.google.com/download?id=10gx1KcFxhVo0aWv-duCyyNe1wKZE3Mo&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.253.115.132, 2607:f8b0:4004:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.253.115.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-02-06 12:19:48 ERROR 404: Not Found.\n",
            "\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_file = './GoogleNews-vectors-negative300.bin.gz'"
      ],
      "metadata": {
        "id": "vm__EXEEBs28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://drive.google.com/uc?export=download&id=10gx1KcFxhVo0aWv-duCyyNe1wKZE3Mo\" -O word2vec_model.bin\n"
      ],
      "metadata": {
        "id": "t6gB1bZd_f6f",
        "outputId": "a363e5e4-c7fe-4803-96a5-5dbc77de6a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-06 12:19:52--  https://drive.google.com/uc?export=download&id=10gx1KcFxhVo0aWv-duCyyNe1wKZE3Mo\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.179.100, 142.251.179.102, 142.251.179.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.179.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=10gx1KcFxhVo0aWv-duCyyNe1wKZE3Mo&export=download [following]\n",
            "--2025-02-06 12:19:52--  https://drive.usercontent.google.com/download?id=10gx1KcFxhVo0aWv-duCyyNe1wKZE3Mo&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.253.115.132, 2607:f8b0:4004:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.253.115.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-02-06 12:19:52 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim==4.*\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" -O word2vec_model.bin.gz\n",
        "!gunzip word2vec_model.bin.gz\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load"
      ],
      "metadata": {
        "id": "XufqetOK_s8i",
        "outputId": "7e77977c-ecd7-4578-f2c9-1908573f2168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.* in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.*) (1.17.2)\n",
            "--2025-02-06 12:19:55--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.12.18, 52.217.93.102, 54.231.225.88, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.12.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-02-06 12:19:55 ERROR 404: Not Found.\n",
            "\n",
            "\n",
            "gzip: word2vec_model.bin.gz: unexpected end of file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim==4.*\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" -O GoogleNews-vectors-negative300.bin.gz\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load the word vectors from the correct file\n",
        "word_vectors = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True, limit=200000)\n",
        "# limit to 200k for speed"
      ],
      "metadata": {
        "id": "27XXA101_49L",
        "outputId": "2cd88d13-9508-4560-b2a7-59b85ed4ae0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.* in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.*) (1.17.2)\n",
            "--2025-02-06 12:20:42--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.15.184.31, 54.231.195.160, 3.5.9.199, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.15.184.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-02-06 12:20:42 ERROR 404: Not Found.\n",
            "\n",
            "\n",
            "gzip: GoogleNews-vectors-negative300.bin.gz: unexpected end of file\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-f18d14c0f4a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the word vectors from the correct file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GoogleNews-vectors-negative300.bin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# limit to 200k for speed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://drive.google.com/file/d/1Ogx1KcfXhVo0aWv-duCyyNelWcKZE3Mo/view?usp=sharing\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V8tuqbOBnwp",
        "outputId": "fd3c11eb-b4fd-4b96-eb69-8a3f0d4bf2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=1Ogx1KcfXhVo0aWv-duCyyNelWcKZE3Mo\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/file/d/1Ogx1KcfXhVo0aWv-duCyyNelWcKZE3Mo/view?usp=sharing\n",
            "To: /content/view?usp=sharing\n",
            "93.4kB [00:00, 7.33MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=200000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "xvHApDOxByc6",
        "outputId": "78be6510-70ff-4fde-9a43-9b15f9b77fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-521a5ea6fd61>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format"
      ],
      "metadata": {
        "id": "f3CSBsv7OhD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Assuming 'embedding_file' contains the path to your word2vec file\n",
        "embedding_file = './GoogleNews-vectors-negative300.bin.gz'  # Or your actual path\n",
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=200000)\n",
        "\n",
        "# Now you can access word vectors:\n",
        "pizza = word_vectors['pizza']\n",
        "print(f'Vector dimension: {pizza.shape}')\n",
        "print(pizza)"
      ],
      "metadata": {
        "id": "aePqR-JfB0j8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "a61e8fdd-096b-4fd6-fb39-1eb4292874ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-bfc97076c995>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming 'embedding_file' contains the path to your word2vec file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./GoogleNews-vectors-negative300.bin.gz'\u001b[0m  \u001b[0;31m# Or your actual path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Now you can access word vectors:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "00m75k_uEK6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8c911b-5c1b-4eef-bdc1-1583ad4212c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim==4.*\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" -O GoogleNews-vectors-negative300.bin.gz\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Assuming 'embedding_file' contains the path to your word2vec file\n",
        "embedding_file = 'GoogleNews-vectors-negative300.bin'  # Updated file path\n",
        "\n",
        "# Load the word vectors from the correct file\n",
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=200000)\n",
        "\n",
        "# Now you can access word vectors:\n",
        "pizza = word_vectors['pizza']\n",
        "print(f'Vector dimension: {pizza.shape}')\n",
        "print(pizza)"
      ],
      "metadata": {
        "id": "CzzRX2zwC_fC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "856a769d-7e86-49ac-fe29-6314eaa644a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.* in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.*) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.*) (1.17.2)\n",
            "--2025-02-06 12:28:17--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.192.184, 52.217.75.158, 3.5.12.82, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.192.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-02-06 12:28:17 ERROR 404: Not Found.\n",
            "\n",
            "\n",
            "gzip: GoogleNews-vectors-negative300.bin.gz: unexpected end of file\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-5ee7bb78cd09>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the word vectors from the correct file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Now you can access word vectors:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://drive.google.com/file/d/1Ogx1KcfXhVo0aWv-duCyyNelWcKZE3Mo/view?usp=sharing\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE4-THmCDIoG",
        "outputId": "d90648a2-8b74-48aa-a054-3f2c7d0b4a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=1Ogx1KcfXhVo0aWv-duCyyNelWcKZE3Mo\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/file/d/1Ogx1KcfXhVo0aWv-duCyyNelWcKZE3Mo/view?usp=sharing\n",
            "To: /content/view?usp=sharing\n",
            "\r0.00B [00:00, ?B/s]\r93.7kB [00:00, 35.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vectors.similarity('pizza', 'tomato'))\n",
        "print(word_vectors.similarity('pizza', 'sauce'))\n",
        "print(word_vectors.similarity('pizza', 'cheese'))"
      ],
      "metadata": {
        "id": "ECvOs0bLNWsw",
        "outputId": "4021b3d0-6cc9-4432-f69c-4e688c7fd966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'similarity'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-fa3fa7f7e994>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pizza'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tomato'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pizza'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sauce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pizza'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cheese'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'similarity'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  word_vectors['womblyboo']\n",
        "except KeyError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "XHdb4qPPOrv0",
        "outputId": "5d428fcb-2619-4827-8b91-b6e5e6413aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'method' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-6dd27e7cbcc0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mword_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'womblyboo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Xsd-tc7Ou-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}